% This file was created with Citavi 6.10.0.0

@book{AAAIPress.2008,
 year = {2008},
 title = {AIIDE'08: Proceedings of the Fourth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
 editor = {{AAAI Press}}
}


@article{Auer.2002,
 author = {Auer, Peter and Cesa-Bianchi, Nicol{\`o} and Fischer, Paul},
 year = {2002},
 title = {Finite-time Analysis of the Multiarmed Bandit Problem},
 pages = {235--256},
 volume = {47},
 number = {2/3},
 journal = {Machine Learning}
}


@book{Badica.2008,
 year = {2008},
 title = {Advances in Intelligent and Distributed Computing},
 url = {http://swbplus.bsz-bw.de/bsz323613896cov.htm},
 address = {Berlin, Heidelberg},
 volume = {78},
 publisher = {{Springer Berlin Heidelberg}},
 series = {SpringerLink B{\"u}cher},
 editor = {Badica, Costin and Paprzycki, Marcin},
 file = {Badica, Paprzycki (Hg.) 2008 - Advances in Intelligent and Distributed:Attachments/Badica, Paprzycki (Hg.) 2008 - Advances in Intelligent and Distributed.pdf:application/pdf}
}


@misc{Baier.2015,
 author = {Baier, Hendrik and Winands, Mark H. M.},
 year = {2015},
 title = {MCTS-Minimax Hybrids},
 file = {Baier, Winands 2015 - MCTS-Minimax Hybrids:Attachments/Baier, Winands 2015 - MCTS-Minimax Hybrids.pdf:application/pdf}
}


@misc{Baier.2018,
 author = {Baier, Hendrik and Winands, Mark H. M.},
 year = {2018},
 title = {MCTS-Minimax Hybrids with State Evaluations},
 file = {Baier, Winands 2018 - MCTS-Minimax Hybrids with State Evaluations:Attachments/Baier, Winands 2018 - MCTS-Minimax Hybrids with State Evaluations.pdf:application/pdf}
}


@misc{Bjorck.01.06.2018,
 abstract = {Batch normalization (BN) is a technique to normalize activations in intermediate layers of deep neural networks. Its tendency to improve accuracy and speed up training have established BN as a favorite technique in deep learning. Yet, despite its enormous success, there remains little consensus on the exact reason and mechanism behind these improvements. In this paper we take a step towards a better understanding of BN, following an empirical approach. We conduct several experiments, and show that BN primarily enables training with larger learning rates, which is the cause for faster convergence and better generalization. For networks without BN we demonstrate how large gradient updates can result in diverging loss and activations growing uncontrollably with network depth, which limits possible learning rates. BN avoids this problem by constantly correcting activations to be zero-mean and of unit standard deviation, which enables larger gradient steps, yields faster convergence and may help bypass sharp local minima. We further show various ways in which gradients and activations of deep unnormalized networks are ill-behaved. We contrast our results against recent findings in random matrix theory, shedding new light on classical initialization schemes and their consequences.},
 author = {Bjorck, Johan and Gomes, Carla and Selman, Bart and Weinberger, Kilian Q.},
 date = {2018},
 title = {Understanding Batch Normalization},
 file = {Bjorck, Gomes et al. 01.06.2018 - Understanding Batch Normalization:Attachments/Bjorck, Gomes et al. 01.06.2018 - Understanding Batch Normalization.pdf:application/pdf}
}


@misc{Bjorck.2018b,
 abstract = {Batch normalization (BN) is a technique to normalize activations in intermediate layers of deep neural networks. Its tendency to improve accuracy and speed up training have established BN as a favorite technique in deep learning. Yet, despite its enormous success, there remains little consensus on the exact reason and mechanism behind these improvements. In this paper we take a step towards a better understanding of BN, following an empirical approach. We conduct several experiments, and show that BN primarily enables training with larger learning rates, which is the cause for faster convergence and better generalization. For networks without BN we demonstrate how large gradient updates can result in diverging loss and activations growing uncontrollably with network depth, which limits possible learning rates. BN avoids this problem by constantly correcting activations to be zero-mean and of unit standard deviation, which enables larger gradient steps, yields faster convergence and may help bypass sharp local minima. We further show various ways in which gradients and activations of deep unnormalized networks are ill-behaved. We contrast our results against recent findings in random matrix theory, shedding new light on classical initialization schemes and their consequences.},
 author = {Bjorck, Johan and Gomes, Carla and Selman, Bart and Weinberger, Kilian Q.},
 date = {2018},
 title = {Understanding Batch Normalization},
 file = {Bjorck, Gomes et al. 01.06.2018 - Understanding Batch Normalization (2):Attachments/Bjorck, Gomes et al. 01.06.2018 - Understanding Batch Normalization (2).pdf:application/pdf}
}


@book{Buchmuller.2016,
 abstract = {Vorwort (Edwin Kreuzer, Pr{\"a}sident der Akademie der Wissenschaften in Hamburg) -- Wilfried Buchm{\"u}ller und Cord Jakobeit: Wissen und Wissenschaft -- Elke Brendel und Ulrich G{\"a}hde: Was ist Wissen? -- Albert Meier: Literaturwissenschaft nach der Postmoderne -- Gabriele Clemens: {\glqq}Scheitert der Euro, dann scheitert Europa{\grqq}? Krisen im europ{\"a}ischen Integrationsprozess -- Anja Pistor-Hatam: {\glqq}Sie kamen, brandschatzten, t{\"o}teten, pl{\"u}nderten und zogen wieder ab{\grqq}. Moderne iranische Geschichtsschreibung und der {\glqq}Mongolensturm{\grqq} im 13. Jahrhundert -- Kurt Pawlik: Psychologische Intelligenzforschung -- Brigitte R{\"o}der und Frank R{\"o}sler: EinBlick in Gehirn und Geist -- Marylin M. Addo und Ansgar W. Lohse: Viren {\"u}berlisten: Globale Virusinfektionen werden beherrschbar- aber neue Gefahren drohen -- Cord Jakobeit: Rohstoffreichtum - Erd{\"o}l als Segen oder Fluch f{\"u}r die Entwicklung?- Mojib Latif: Treibhauseffekt, Wetter, Klima, Klimawandel -- Franz Joos: Wissenschaft und Forschung zur effizienten, umweltfreundlichen Energiewandlung -- Horst Weller: Small is beautiful: Nanopartikel f{\"u}r technologische und medizinische Forschung und Anwendung -- Reiner Lauterbach: Wie das Chaos in die Welt kam -- Wilfried Buchm{\"u}ller: Das Higgs-Teilchen und der Ursprung der Materie


Dieser Band gew{\"a}hrt einen einzigartigen Einblick in Geistes- und Naturwissenschaften heute. Renommierte Autorinnen und Autoren beantworten grundlegende Fragen wie: Was ist Wissenschaft, und wie beeinflusst sie unser Welt- und Menschenbild? Wie gelangt Forschung zu Erkenntnissen, und wie werden diese Erkenntnisse in die Gesellschaft eingebracht? Was sind die Antriebskr{\"a}fte der Wissenschaft? Und wie steht es um den Wahrheitsgehalt des gewonnenen Wissens? Gibt es Kriterien zur Beurteilung der Qualit{\"a}t von Wissen? Und wie ist das Verh{\"a}ltnis von Information, Wissen und Erkenntnis? Der Band leistet einen Beitrag dazu, die gro{\ss}e Erweiterung unseres Wissens heute verst{\"a}ndlich zu vermitteln und gleichzeitig auf die Begrenztheit dieses Wissens hinzuweisen. Forschung wird getrieben von Neugierde, von {\glqq}Lust an der Erkenntnis{\grqq}. Dies hat zu einer enormen Vielfalt von Forschungsrichtungen gef{\"u}hrt, wobei Geistes-, Sozial-, Natur- und Ingenieurwissenschaften verschiedene Facetten desselben Strebens nach Erkenntnis sind. Argumentiert wird, dass es trotz der Vielfalt der Disziplinen und der Ausdifferenzierung in den Disziplinen eine gemeinsame {\glqq}Kultur der Wissenschaft{\grqq} gibt. Sie wird wesentlich bestimmt durch grundlegende methodische Standards, die alle Disziplinen erf{\"u}llen m{\"u}ssen, um als Wissenschaften gelten zu k{\"o}nnen. Die Faszination f{\"u}r Forschung ist ungebrochen und ihre Ergebnisse werden unsere Gesellschaft und unser Welt- und Menschenbild immer st{\"a}rker pr{\"a}gen. Die Herausgeber Wilfried Buchm{\"u}ller ist Leitender Wissenschaftler am Deutschen Elektronen-Synchrotron DESY und Professor f{\"u}r theoretische Elementarteilchenphysik an der Universit{\"a}t Hamburg. Sein Hauptarbeitsgebiet ist die Schnittstelle zwischen Teilchenphysik und Kosmologie. Cord Jakobeit ist Professor f{\"u}r Politikwissenschaft, insbesondere Internationale Beziehungen, an der Universit{\"a}t Hamburg. Sein Hauptarbeitsgebiet sind die internationalen Wirtschaftsbeziehungen. Er ist Vizepr{\"a}sident der Akademie der Wissenschaften in Hamburg},
 year = {2016},
 title = {Erkenntnis, Wissenschaft und Gesellschaft: Wie Forschung Wissen schafft},
 address = {Berlin, Heidelberg},
 publisher = {{Springer Berlin Heidelberg}},
 editor = {Buchm{\"u}ller, Wilfried and Jakobeit, Cord},
 file = {Buchm{\"u}ller, Jakobeit (Hg.) 2016 - Erkenntnis:Attachments/Buchm{\"u}ller, Jakobeit (Hg.) 2016 - Erkenntnis.pdf:application/pdf}
}


@book{CameronBrowne.2012,
 author = {Browne, Cameron B. and Powley, Edward and Whitehouse, Daniel and Lucas, Simon M. and Cowling, Peter I. and Rohlfshagen, Philipp and Tavener, Stephen and Perez, Diego and Samothrakis, Spyridon and Colton, Simon},
 year = {2012},
 title = {A Survey of Monte Carlo Tree Search Methods},
 journal = {IEEE Transactions on Computational Intelligence and AI in Games},
 file = {Browne, Powley et al. 2012 - A Survey of Monte Carlo:Attachments/Browne, Powley et al. 2012 - A Survey of Monte Carlo.pdf:application/pdf}
}


@misc{CarstenKern.2021,
 author = {Kern, Carsten},
 date = {2021},
 title = {Vorlesung: Client-K.I.s f{\"u}r Brettspiele},
 address = {Regensburg},
 institution = {{Ostbayerische Technische Hochschule Regensburg}}
}


@misc{Cooijmans.30.03.2016,
 abstract = {We propose a reparameterization of LSTM that brings the benefits of batch normalization to recurrent neural networks. Whereas previous works only apply batch normalization to the input-to-hidden transformation of RNNs, we demonstrate that it is both possible and beneficial to batch-normalize the hidden-to-hidden transition, thereby reducing internal covariate shift between time steps. We evaluate our proposal on various sequential problems such as sequence classification, language modeling and question answering. Our empirical results show that our batch-normalized LSTM consistently leads to faster convergence and improved generalization.},
 author = {Cooijmans, Tim and Ballas, Nicolas and Laurent, C{\'e}sar and G{\"u}l{\c{c}}ehre, {\c{C}}a{\u{g}}lar and Courville, Aaron},
 date = {2016},
 title = {Recurrent Batch Normalization},
 file = {Cooijmans, Ballas et al. 30.03.2016 - Recurrent Batch Normalization:Attachments/Cooijmans, Ballas et al. 30.03.2016 - Recurrent Batch Normalization.pdf:application/pdf}
}


@misc{EmmanuelLazard.1993,
 author = {{Emmanuel Lazard}},
 year = {1993},
 title = {OTHELLO: THE RULES OF THE GAME},
 urldate = {13.09.2021}
}


@book{Ferguson.2019,
 abstract = {Intro -- Deep Learning and the Game of Go -- Max Pumperla and Kevin Ferguson -- Copyright -- Dedication -- Brief Table of Contents -- Table of Contents -- front matter -- Foreword -- Preface -- Acknowledgments -- About this book -- Who should read this book -- Roadmap -- About the code -- Book forum -- About the authors -- About the cover illustration -- Part 1. Foundations -- 1 Toward deep learning: a machine-learning introduction -- 1.1. What is machine learning? -- 1.1.1. How does machine learning relate to AI? -- 1.1.2. What you can and can't do with machine learning -- 1.2. Machine learning by example -- 1.2.1. Using machine learning in software applications -- 1.2.2. Supervised learning -- 1.2.3. Unsupervised learning -- 1.2.4. Reinforcement learning -- 1.3. Deep learning -- 1.4. What you'll learn in this book -- 1.5. Summary -- 2 Go as a machine-learning problem -- 2.1. Why games? -- 2.2. A lightning introduction to the game of Go -- 2.2.1. Understanding the board -- 2.2.2. Placing and capturing stones -- 2.2.3. Ending the game and counting -- 2.2.4. Understanding ko -- 2.3. Handicaps -- 2.4. Where to learn more -- 2.5. What can we teach a machine? -- 2.5.1. Selecting moves in the opening -- 2.5.2. Searching game states -- 2.5.3. Reducing the number of moves to consider -- 2.5.4. Evaluating game states -- 2.6. How to measure your Go AI's strength -- 2.6.1. Traditional Go ranks -- 2.6.2. Benchmarking your Go AI -- 2.7. Summary -- 3 Implementing your first Go bot -- 3.1. Representing a game of Go in Python -- 3.1.1. Implementing the Go board -- 3.1.2. Tracking connected groups of stones in Go: strings -- 3.1.3. Placing and capturing stones on a Go board -- 3.2. Capturing game state and checking for illegal moves -- 3.2.1. Self-capture -- 3.2.2. Ko -- 3.3. Ending a game -- 3.4. Creating your first bot: the weakest Go AI imaginable.},
 author = {Ferguson, Kevin},
 year = {2019},
 title = {Deep Learning and the Game of Go},
 address = {New York},
 publisher = {{Manning Publications Co. LLC}},
 edition = {1. Auflage}
}


@book{Frochte.2019,
 author = {Frochte, J{\"o}rg},
 year = {2019},
 title = {Maschinelles Lernen: Grundlagen und Algorithmen in Python},
 price = {EUR 38.00 (DE), EUR 39.10 (AT)},
 address = {M{\"u}nchen},
 edition = {2. Auflage},
 publisher = {Hanser},
}


@book{Fujita.2016,
 year = {2016},
 title = {Trends in applied knowledge-based systems and data science: 29th International Conference on Industrial Engineering and Other Applications of Applied Intelligent Systems, IEA/AIE 2016, Morioka, Japan, August 2-4, 2016 : proceedings},
 address = {Cham},
 volume = {9799},
 publisher = {Springer},
 series = {Lecture notes in computer science Lecture notes in artificial intelligence},
 editor = {Fujita, Hamido and Ali, Moonis and Selamat, Ali and Kurematsu, Masaki},
}


@book{gamma.2011,
 author = {{Erich Gamma} and {Richard Helm} and {Ralph Johnson} and {John Vlissides}},
 year = {2011},
 title = {Design patterns: Elements of reusable object-oriented software},
 address = {Boston},
 edition = {39. Auflage},
 publisher = {Addison-Wesley},
 series = {Addison-Wesley professional computing series}
}


@book{Glonnegger.2009,
 abstract = {Die farbenpr{\"a}chtig und aufwendig gestaltete Neubearbeitung des 1988 erstmals erschienenen und 1999 erg{\"a}nzten und erweiterten Titels l{\"a}sst keinerlei W{\"u}nsche zum Thema Spiele offen. Zahlreiche bekannte und unbekannte Spiele werden bestens und {\"u}bersichtlich erkl{\"a}rt sowie deren kulturgeschichtliche Hintergr{\"u}nde dargestellt. Zum Ausprobieren und Improvisieren laden die umfassenden Beschreibungen der Spiele aus verschiedenen Kulturkreisen ein. Dazu tragen auch die hervorragenden Illustrationen bei, die teilweise den Originalspielen, aber auch alten Darstellungen entnommen sind. Der Band mit Brett- und Legespielen aus aller Welt weckt die Lust aufs Spielen und wird sehr gern empfohlen. Die 2. Auflage von 1999 kann, muss aber nicht zwingend ausgetauscht werden. (2) (LK/KET: Breuner)


Umfangreicher und farbenpr{\"a}chtig illustrierter {\"U}berblick {\"u}ber Brett- und Legespiele aus aller Welt, deren Herkunft, Geschichte sowie Spielregeln. (LK/KET: Breuner)},
 author = {Glonnegger, Erwin and R{\"u}ttinger, Johann},
 year = {2009},
 title = {Das Spiele-Buch: Brett- und Legespiele aus aller Welt ; Herkunft, Regeln und Geschichte},
 address = {Ravensburg},
 edition = {Neue, unter Mitarb. von Claus Voigt aktualisierte Aufl.},
 publisher = {{Ravensburger Buchverl. Maier}},
}


@book{Goll.2014,
 abstract = {Prinzipien f{\"u}r den objektorientierten Entwurf -- Softwarearchitekturen -- Muster beim Softwareentwurf -- Objektorientierte Entwurfsmuster -- Architekturmuster.



Architekturen von Softwaresystemen sollen einfach erweiterbar und weitestgehend standardisiert sein, damit die Entwickler sich leicht {\"u}ber Architekturen austauschen k{\"o}nnen. F{\"u}r den objektorientierten Entwurf haben sich zahlreiche wertvolle Architektur- und Entwurfsmuster herausgebildet. Diese Muster basieren auf objektorientier­­ten Prinzipien wie dem Prinzip der Dependency Inversion. Daher werden zuerst die wichtig­sten objektorientierten Prinzipien erkl{\"a}rt. Anschlie{\ss}end wird gezeigt, wie diese objektorientierten Prinzipien in den verschie­denen Architektur- und Entwurfs­mustern umgesetzt werden. Alle vorgestellten Mus­ter werden durch lauff{\"a}hige Bei­spiele in Java illustriert. ~ Der Inhalt Prinzipien f{\"u}r den objektorientierten Entwurf: ~Abstraktion, Kapselung und Information Hiding - Separation of Concerns und das Single Responsibility-Prinzip - Interface Segregation-Prinzip - Loose Coupling - Liskovsches Substitutionsprinzip - Design by Contract - Open-Closed-Prinzip - Dependency Inversion-Prinzip - Softwarearchitekturen: Definition des Begriffs Softwarearchitektur - Rolle des Softwarearchitekten - Qualit{\"a}ten einer Softwarearchitektur - Referenzarchitekturen - Aufga­ben und Sichten bei der Konzeption einer Softwarearchitektur - Muster beim Softwareentwurf: Einsatz~und Eigenschaften von Mustern - Abgren­zung zwischen Architekturmustern, Entwurfsmustern und Idiomen - Schema f{\"u}r die Beschreibung von Entwurfs- und Architekturmustern - Objektorientierte Entwurfsmuster: Klassifikation von Entwurfsmustern - Die Struk­turmuster Adapter, Br{\"u}cke, Dekorierer, Fassade, Kompositum und Proxy - Die Verhaltensmuster Schablonenmethode, Befehl, Beobachter, Strategie, Vermittler, Zu­stand, Rolle, Besucher und Iterator - Die Erzeugungsmuster Fabrikmethode, Ab­strak­te Fabrik, Singleton und Objektpool - Architekturmuster: Layers, Pipes and Filters, Plug-in, Broker, Service-Oriented Ar­chitecture, Model-View-Controller ~ Die Zielgruppen Studierende der Informatik und der ingenieurswissenschaftlichen Disziplinen Berufliche Umsteiger und Entwickler in der Praxis Der Autor Professor Dr. Joachim Goll lehrt und forscht an der Hochschule Esslingen.},
 author = {Goll, Joachim},
 year = {2014},
 title = {Architektur- und Entwurfsmuster der Softwaretechnik: Mit lauff{\"a}higen Beispielen in Java},
 address = {Wiesbaden},
 edition = {2. Auflage},
 publisher = {{Springer Vieweg}},
 file = {Goll 2014 - Architektur- und Entwurfsmuster der Softwaretechnik:Attachments/Goll 2014 - Architektur- und Entwurfsmuster der Softwaretechnik.pdf:application/pdf}
}


@book{Goodfellow.2018,
 abstract = {Intro -- Impressum -- Website zum Buch -- Danksagung -- {\"U}ber die Fachkorrektoren zur deutschen Ausgabe -- Notation -- Einleitung -- F{\"u}r wen ist dieses Buch gedacht? -- Historische Entwicklungen im Deep Learning -- I Angewandte Mathematik und Grundlagen f{\"u}r das Machine Learning -- Lineare Algebra -- Skalare, Vektoren, Matrizen und Tensoren -- Multiplizieren von Matrizen und Vektoren -- Einheits- und Umkehrmatrizen -- Lineare Abh{\"a}ngigkeit und lineare H{\"u}lle -- Normen -- Spezielle Matrizen und Vektoren -- Eigenwertzerlegung -- Singul{\"a}rwertzerlegung -- Die Moore-Penrose-Pseudoinverse -- Der Spuroperator -- Die Determinante -- Beispiel: Hauptkomponentenanalyse -- Wahrscheinlichkeits- und Informationstheorie -- Warum Wahrscheinlichkeit? -- Zufallsvariablen -- Wahrscheinlichkeitsverteilungen -- Randwahrscheinlichkeit -- Bedingte Wahrscheinlichkeit -- Die Produktregel der bedingten Wahrscheinlichkeiten -- Unabh{\"a}ngigkeit und bedingte Unabh{\"a}ngigkeit -- Erwartungswert, Varianz und Kovarianz -- H{\"a}ufig genutzte Wahrscheinlichkeitsverteilungen -- N{\"u}tzliche Eigenschaften h{\"a}ufig verwendeter Funktionen -- Satz von Bayes -- Technische Einzelheiten stetiger Variablen -- Informationstheorie -- Strukturierte probabilistische Modelle -- Numerische Berechnung -- {\"U}berlauf und Unterlauf -- Schlechte Konditionierung -- Optimierung auf Gradientenbasis -- Optimierung unter Nebenbedingungen -- Beispiel: Lineare kleinste Quadrate -- Grundlagen f{\"u}r das Machine Learning -- Lernalgorithmen -- Kapazit{\"a}t, {\"U}beranpassung und Unteranpassung -- Hyperparameter und Validierungsdaten -- Sch{\"a}tzer, Verzerrung und Varianz -- Maximum-Likelihood-Sch{\"a}tzung -- Bayessche Statistik -- Algorithmen f{\"u}r {\"u}berwachtes Lernen -- Algorithmen f{\"u}r un{\"u}berwachtes Lernen -- Stochastisches Gradientenabstiegsverfahren -- Entwickeln eines Machine-Learning-Algorithmus -- Probleme, an denen Deep Learning w{\"a}chst.},
 author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
 year = {2018},
 title = {Deep Learning: Das umfassende Handbuch : Grundlagen, aktuelle Verfahren und Algorithmen, neue Forschungsans{\"a}tze},
 address = {Frechen},
 edition = {1. Auflage},
 publisher = {mitp}
}

@article{Gasser.1996,
 author = {Gasser, Ralph},
 year = {1996},
 title = {Solving Nine Men's Morris},
 pages = {24--41},
 volume = {12},
 number = {1},
 journal = {Computational Intelligence},
 file = {Gasser 1996 - SOLVING NINE MEN'S MORRIS:Attachments/Gasser 1996 - SOLVING NINE MEN'S MORRIS.pdf:application/pdf}
}

@misc{GuillaumeChaslot.2008,
 author = {{Guillaume Chaslot} and {Sander Bakkes} and {Istvan Szita} and {Pieter Spronck}},
 title = {Monte-Carlo Tree Search: A New Framework for Game AI},
 year = {2008}
}


@misc{He.2015,
 abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.  The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
 author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
 date = {2015},
 title = {Deep Residual Learning for Image Recognition},
 file = {He, Zhang et al. 10.12.2015 - Deep Residual Learning for Image:Attachments/He, Zhang et al. 10.12.2015 - Deep Residual Learning for Image.pdf:application/pdf}
}


@book{Heinemann.2016,
 abstract = {This book provides efficient code solutions in several programming languages that you can easily adapt to a specific project. Each major algorithm is presented in the style of a design pattern that includes information to help you understand why and when the algorithm is appropriate--



Thinking in algorithms -- The mathematics of algorithms -- Algorithm building blocks -- Sorting algorithms -- Searching -- Graph algorithms -- Path finding in AI -- Network flow algorithms -- Computational geometry -- Spatial tree structures -- Emerging algorithm categories -- Principles of algorithms -- Benchmarking},
 author = {Heinemann, George T. and Heineman, George T. and Pollice, Gary and Selkow, Stanley},
 year = {2016},
 title = {Algorithms in a Nutshell},
 address = {Beijing},
 edition = {2. Auflage},
 publisher = {O'Reilly}
}


@misc{Hinton.03.07.2012,
 abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This {\textquotedbl}overfitting{\textquotedbl} is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random {\textquotedbl}dropout{\textquotedbl} gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
 author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
 date = {2012},
 title = {Improving neural networks by preventing co-adaptation of feature  detectors},
 file = {Hinton, Srivastava et al. 03.07.2012 - Improving neural networks by preventing:Attachments/Hinton, Srivastava et al. 03.07.2012 - Improving neural networks by preventing.pdf:application/pdf}
}

@misc{Irving.03.04.2014,
 author = {Irving, Geoffrey},
 date = {2014},
 title = {Pentago is a First Player Win: Strongly Solving a Game Using Parallel  In-Core Retrograde Analysis},
 file = {Irving 03.04.2014 - Pentago is a First Player:Attachments/Irving 03.04.2014 - Pentago is a First Player.pdf:application/pdf}
}


@book{Jones.2008,
 author = {Jones, M. Tim},
 year = {2008},
 title = {Artificial intelligence: A systems approach},
 address = {Hingham, Mass},
 publisher = {{Infinity Science Press}}
 }


@article{Khan.2020,
 author = {Khan, Asifullah and Sohail, Anabia and Zahoora, Umme and Qureshi, Aqsa Saeed},
 year = {2020},
 title = {A survey of the recent architectures of deep convolutional neural networks},
 pages = {5455--5516},
 volume = {53},
 number = {8},
 journal = {Artificial Intelligence Review},
 file = {Khan, Sohail et al. 2020 - A survey of the recent:Attachments/Khan, Sohail et al. 2020 - A survey of the recent.pdf:application/pdf}
}


@book{Krohn.2020,
 abstract = {Deep Learning begreifen und einsetzen. Einf{\"u}hrung in verwandte Themen wie K{\"u}nstliche Intelligenz, Machine Learning und Neuronale Netze; viele Illustrationen, verst{\"a}ndlich erkl{\"a}rt; begleitendes online-Material zum Ausprobieren der Erl{\"a}uterungen aus dem Buch (Jupyter-Notebooks); Vorstellung von Bibliotheken (Tensor Flow/Keras, PyTorch). Deep Learning ver{\"a}ndert unseren Alltag. Dieser Ansatz f{\"u}r maschinelles Lernen erzielt bahnbrechende Ergebnisse in einigen der bekanntesten Anwendungen von heute, in Unternehmen von Google bis Tesla, Facebook bis Apple. Tausende von technischen Fachkr{\"a}ften und Studenten wollen seine M{\"o}glichkeiten einsetzen, aber fr{\"u}here B{\"u}cher {\"u}ber Deep Learning waren oft nicht intuitiv, unzug{\"a}nglich und trocken. John Krohn, Grant Beylefeld und Agla{\'e} Bassens bieten Ihnen eine einzigartige visuelle, intuitive und verst{\"a}ndliche Einf{\"u}hrung in Techniken und Anwendungen von Deep Learning. Mit den farbenfrohen Illustrationen und eing{\"a}ngigen Erl{\"a}uterungen von {\textquotedbl}Deep Learning illustriert{\textquotedbl} gelingt Ihnen ein einfacher Zugang zum Aufbau von Deep-Learning-Modellen, und bringt ihnen beim Lernen mehr Spa{\ss}. Der erste Teil des Buches erkl{\"a}rt, was Deep Learning ist, warum es so allgegenw{\"a}rtig geworden ist und wie es mit Konzepten und Terminologien wie k{\"u}nstlicher Intelligenz, Machine Learning oder k{\"u}nstlichen neuronalen Netzen interagiert. Dabei verwenden die Autoren leicht verst{\"a}ndliche Analogien, lebendige Grafiken und viele Beispiele. Auf dieser Grundlage pr{\"a}sentieren die Autoren eine praktische Referenz und ein Tutorial zur Anwendung eines breiten Spektrums bew{\"a}hrter Techniken des Deep Learning. Die wesentliche Theorie wird mit so wenig Mathematik wie m{\"o}glich behandelt und mit praktischem Python-Code beleuchtet. Praktische Beispiele zum Ausprobieren, die kostenfrei online verf{\"u}gbar sind (Jupyter-Notebooks), machen Ihnen die Theorie begreiflich. So erlangen Sie ein pragmatisches Verst{\"a}ndnis aller wichtigen Deep-Learning-Ans{\"a}tze und ihrer Anwendungen: Machine Vision, Natural Language Processing, Bilderzeugung und Spielalgorithmen. Um Ihnen zu helfen, mehr in k{\"u}rzerer Zeit zu erreichen, stellen die Autoren mehrere der heute am weitesten verbreiteten und innovativsten Deep-Learning-Bibliotheken vor, darunter: TensorFlow und seine High-Level-API, Keras; PyTorch; High-Level-Coach, eine TensorFlow-API, die die Komplexit{\"a}t, die typischerweise mit der Entwicklung von Deep Reinforcement Learning-Algorithmen verbunden ist, abstrahiert.},
 author = {Krohn, Jon and Beyleveld, Grant and Bassens, Agla{\'e}},
 year = {2020},
 title = {Deep Learning illustriert: Eine anschauliche Einf{\"u}hrung in Machine Vision, Natural Language Processing und Bilderzeugung f{\"u}r Programmierer und Datenanalysten},
 address = {Heidelberg},
 publisher = {dpunkt.ver-lag},
 institution = {Dpunkt.Verlag},
 edition = {1. Auflage}
}


@book{Kubat.2021,
 abstract = {1. Ambitions and Goals of Machine Learning -- 2. Probabilities: Bayesian Classifiers -- 3. Similarities: Nearest-Neighbor Classifiers -- 4. Inter-Class Boundaries: Linear and Polynomial Classifiers -- 5. Decision Trees -- 6. Artificial Neural Networks -- 7. Computational Learning Theory -- 8. Experience from Historical Applications -- 9. Voting Assemblies and Boosting -- 10. Classifiers in the Form of Rule-Sets -- 11. Practical Issues to Know About -- 12. Performance Evaluation -- 13. Statistical Significance -- 14. Induction in Multi-Label Domains -- 15. Unsupervised Learning -- 16. Deep Learning -- 17. Reinforcement Learning: N-Armed Bandits and Episodes -- 18. Reinforcement Learning: From TD(0) to Deep-Q-Learning -- 19. Temporal Learning -- 20. Hidden Markov Models -- 21. Genetic Algorithm -- Bibliography -- Index.



This textbook offers a comprehensive introduction to Machine Learning techniques and algorithms. This Third Edition covers newer approaches that have become highly topical, including deep learning, and auto-encoding, introductory information about temporal learning and hidden Markov models, and a much more detailed treatment of reinforcement learning. The book is written in an easy-to-understand manner with many examples and pictures, and with a lot of practical advice and discussions of simple applications. The main topics include Bayesian classifiers, nearest-neighbor classifiers, linear and polynomial classifiers, decision trees, rule-induction programs, artificial neural networks, support vector machines, boosting algorithms, unsupervised learning (including Kohonen networks and auto-encoding), deep learning, reinforcement learning, temporal learning (including long short-term memory), hidden Markov models, and the genetic algorithm. Special attention is devoted to performance evaluation, statistical assessment, and to many practical issues ranging from feature selection and feature construction to bias, context, multi-label domains, and the problem of imbalanced classes.},
 author = {Kubat, Miroslav},
 year = {2021},
 title = {An Introduction to Machine Learning},
 address = {Cham},
 edition = {3. Auflage},
 publisher = {{Springer International Publishing} and {Imprint Springer}}
 }


@misc{Liskowski.2018,
 author = {Liskowski, Pawel and Jaskowski, Wojciech and Krawiec, Krzysztof},
 year = {2018},
 title = {Learning to Play Othello With Deep Neural Networks},
 file = {Liskowski, Jaskowski et al. 2018 - Learning to Play Othello:Attachments/Liskowski, Jaskowski et al. 2018 - Learning to Play Othello.pdf:application/pdf}
}


@book{NikLever.2012,
 author = {{Nik Lever}},
 year = {2012},
 title = {Director MX 2004 Games: Game Development with Macromedia Director},
 address = {London},
 edition = {1. Auflage},
 publisher = {{Taylor {\&} Francis}}
}


@book{Nystrom.2014,
 author = {Nystrom, Robert},
 year = {2014},
 title = {Game programming patterns},
 edition = {1. Auflage},
 publisher = {{genever benning}}
}


@incollection{Paraschiv.2008,
 author = {Paraschiv, Daniel and Vasiliu, Laurentiu},
 title = {Learn Reversi using Parallel Genetic Algorithms},
 pages = {295--301},
 volume = {78},
 publisher = {{Springer Berlin Heidelberg}},
 series = {SpringerLink B{\"u}cher},
 editor = {Badica, Costin and Paprzycki, Marcin},
 booktitle = {Advances in Intelligent and Distributed Computing},
 year = {2008},
 address = {Berlin, Heidelberg},
 file = {Paraschiv, Vasiliu 2008 - Learn Reversi using Parallel Genetic:Attachments/Paraschiv, Vasiliu 2008 - Learn Reversi using Parallel Genetic.pdf:application/pdf}
}


@misc{Pascanu.2012,
 abstract = {There are two widely known issues with properly training Recurrent Neural Networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
 author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
 date = {2012},
 title = {On the difficulty of training Recurrent Neural Networks},
 file = {Pascanu, Mikolov et al. 21.11.2012 - On the difficulty of training:Attachments/Pascanu, Mikolov et al. 21.11.2012 - On the difficulty of training.pdf:application/pdf}
}


@misc{Polk.2016,
 author = {Polk, Spencer and Oommen, B. John},
 title = {Challenging Established Move Ordering Strategies with Adaptive Data Structures},
 year = {2016},
 file = {Polk, Oommen 2016 - Challenging Established Move Ordering Strategies:Attachments/Polk, Oommen 2016 - Challenging Established Move Ordering Strategies.pdf:application/pdf}
}


@book{Reinefeld.1989,
 abstract = {Baum-Suchverfahren werden in der Informatik, insbesondere im Teilbereich der K{\"u}nstlichen Intelligenz, zum Durchsuchen von Entscheidungsb{\"a}umen eingesetzt. Das vorliegende Buch befa{\ss}t sich mit Baum-Suchverfahren f{\"u}r eine spezielle Art von Entscheidungsb{\"a}umen, den Spielb{\"a}umen. Es werden zwei grundlegende Klassen von Spielbaum-Suchverfahren ausf{\"u}hrlich behandelt: die Nullfenster-Suchverfahren, die den Baum in einer vorher festgelegten Reihenfolge durchsuchen, und die Zustandsraum-Suchverfahren, deren Suchabfolge dynamisch gesteuert ist. Der praktisch orientierte Spielprogrammierer findet in diesem Buch einen universell verwendbaren Grundstock von Baum-Suchalgorithmen f{\"u}r Zwei-Personen-Null-Summen-Spiele, wie z.B. Schach, Dame und Go. Neben den Algorithmen selbst werden ihm theoretische und empirische Bewertungskriterien an die Hand gegeben, mit denen er die zu erwartende Suchleistung eines Algorithmus absch{\"a}tzen kann. Der an den theoretischen Grundlagen der Spielbaumsuche interessierte Leser findet in diesem Buch Ans{\"a}tze zur Analyse der Suchabfolge und zur Berechnung der Sucheffizienz der Algorithmen. Den Ausgangspunkt bilden dabei die zu durchsuchenden B{\"a}ume, deren Knotenbeziehungen auf einfache Weise in mathematischen Gleichungssystemen beschrieben werden},
 author = {Reinefeld, Alexander},
 year = {1989},
 title = {Spielbaum-Suchverfahren},
 address = {Berlin and Heidelberg},
 publisher = {Springer}
 }


@book{Russell.2012,
 abstract = {This book introduces the BOXES methodology. It also presents a generic BOXES coefficient that makes the system almost completely application-independent, eliminating much of the a priori system knowledge currently needed for the method to be possible.



Intro -- The BOXES Methodology -- Donald Michie: A Personal Appreciation -- Foreword -- Acknowledgments -- Contents -- 1 Introduction -- 1.1$\ldots$Machine Intelligence -- 1.1.1 Are Computers Intelligent? -- 1.1.2 Can Computers Learn? -- 1.1.3 The Robot and the Box -- 1.1.4 Does the BOXES Algorithm Learn? -- 1.1.5 Can Computers Think? -- 1.1.6 Does the BOXES Algorithm Think? -- 1.2$\ldots$The Purpose of this Book -- 1.3$\ldots$A Roadmap to Reading this Book -- 1.4$\ldots$Concluding Thoughts -- Part I Learning and Artificial Intelligence (AI) -- 2 The Game Metaphor -- 2.1$\ldots$Computers can be Programed to Play Games -- 2.1.1 Playing by Rules -- 2.2$\ldots$Reactionary Strategy Games -- 2.2.1 Noughts and Crosses -- 2.2.2 OXO not Noughts and Crosses -- 2.3$\ldots$Incentives and Learning Velocity -- 2.4$\ldots$Design of a Noughts and Crosses Engine -- 2.4.1 Overview -- 2.4.2 Software Sub-Structures -- 2.4.3 Typical Results -- 2.5$\ldots$Chance and Trial and Error -- 2.5.1 Chance and the Community Chest -- 2.5.2 Learning with Guesswork -- 2.5.3 Random Initialization -- 2.5.4 Positional Move Strengths -- 2.6$\ldots$The Payoff Matrix -- 2.7$\ldots$The Signature Table -- 2.8$\ldots$Rewards and Penalties -- 2.9$\ldots$Failure Driven LearningFailure driven learning -- 2.10$\ldots$Concluding Thoughts -- 2.10.1 Reversi (Othelloreg) -- 2.10.2 The BOXES Method as a Game -- References -- 3 Introduction to BOXES Control -- 3.1$\ldots$Matchboxes -- 3.2$\ldots$Components of the BOXES Method -- 3.2.1 Defining the Game Board -- 3.2.2 Identifying Game Situations -- 3.2.3 Selecting Game Piece Actions -- 3.2.4 Real-Time Data Handling -- 3.2.5 Detecting an End Game Situation -- 3.3$\ldots$Updating the Signature Table -- 3.3.1 Overall Performance Data -- 3.3.2 Desired Level of AchievementDesired Level of Achievement -- 3.3.3 Individual Box Decision Data -- 3.4$\ldots$Overall Software Design -- 3.5$\ldots$Concluding Comments -- References -- 4 Dynamic ControlDynamic control as a Game.},
 author = {Russell, David W.},
 year = {2012},
 title = {The BOXES Methodology: Black Box Dynamic Control},
 address = {London},
 edition = {1. Auflage},
 publisher = {{Springer London Limited}}
 }


@misc{Shantanu.2018,
 author = {{Shantanu Thakoor} and {Surag Nair} and {Megha Jhunjhunwala}},
 date = {2018},
 title = {Learning to Play Othello Without Human Knowledge}
}


@misc{Silver.05.12.2017,
 abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
 author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
 date = {2017},
 title = {Mastering Chess and Shogi by Self-Play with a General Reinforcement  Learning Algorithm},
 file = {Silver, Hubert et al. 05.12.2017 - Mastering Chess and Shogi:Attachments/Silver, Hubert et al. 05.12.2017 - Mastering Chess and Shogi.pdf:application/pdf}
}


@misc{Silver.2017,
 abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100-0 against the previously published, champion-defeating AlphaGo.},
 author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and {van den Driessche}, George and Graepel, Thore and Hassabis, Demis},
 year = {2017},
 title = {Mastering the game of Go without human knowledge},
 file = {Silver, Schrittwieser et al. 2017 - Mastering the game of Go:Attachments/Silver, Schrittwieser et al. 2017 - Mastering the game of Go.pdf:application/pdf}
}


@book{Sutton.2018,
 abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence.



Intro -- Series Page -- Title Page -- Copyright -- Dedication -- Table of Contents -- Preface to the Second Edition -- Preface to the First Edition -- Summary of Notation -- 1. Introduction -- 1.1. Reinforcement Learning -- 1.2. Examples -- 1.3. Elements of Reinforcement Learning -- 1.4. Limitations and Scope -- 1.5. An Extended Example: Tic-Tac-Toe -- 1.6. Summary -- 1.7. Early History of Reinforcement Learning -- I: Tabular Solution Methods -- 2. Multi-armed Bandits -- 2.1. A k-armed Bandit Problem -- 2.2. Action-value Methods -- 2.3. The 10-armed Testbed -- 2.4. Incremental Implementation -- 2.5. Tracking a Nonstationary Problem -- 2.6. Optimistic Initial Values -- 2.7. Upper-Confidence-Bound Action Selection -- 2.8. Gradient Bandit Algorithms -- 2.9. Associative Search (Contextual Bandits) -- 2.10 Summary -- 3. Finite Markov Decision Processes -- 3.1. The Agent-Environment Interface -- 3.2. Goals and Rewards -- 3.3. Returns and Episodes -- 3.4. Unified Notation for Episodic and Continuing Tasks -- 3.5. Policies and Value Functions -- 3.6. Optimal Policies and Optimal Value Functions -- 3.7. Optimality and Approximation -- 3.8. Summary -- 4. Dynamic Programming -- 4.1. Policy Evaluation (Prediction) -- 4.2. Policy Improvement -- 4.3. Policy Iteration -- 4.4. Value Iteration -- 4.5. Asynchronous Dynamic Programming -- 4.6. Generalized Policy Iteration -- 4.7. Efficiency of Dynamic Programming -- 4.8. Summary -- 5. Monte Carlo Methods -- 5.1. Monte Carlo Prediction -- 5.2. Monte Carlo Estimation of Action Values -- 5.3. Monte Carlo Control -- 5.4. Monte Carlo Control without Exploring Starts -- 5.5. Off-policy Prediction via Importance Sampling -- 5.6. Incremental Implementation -- 5.7. Off-policy Monte Carlo Control -- 5.8. *Discounting-aware Importance Sampling -- 5.9. *Per-decision Importance Sampling -- 5.10. Summary.},
 author = {Sutton, Richard S. and Barto, Andrew},
 year = {2018},
 title = {Reinforcement learning, second edition: An introduction},
 address = {Cambridge},
 edition = {2. Auflage},
 publisher = {{MIT Press}}
 }


@book{UweLammel.2020,
 author = {{Uwe L{\"a}mmel} and {J{\"u}rgen Cleve}},
 year = {2020},
 title = {K{\"u}nstliche Intelligenz: Wissensverarbeitung -- Neuronale Netze},
 address = {M{\"u}nchen},
 edition = {5. Auflage},
 publisher = {{Carl Hanser Verlag GmbH {\&} Co. KG}}
 }


@book{VaishnaviSannidhanam.2015,
 author = {{Vaishnavi Sannidhanam} and {Muthukaruppan Annamalai}},
 year = {2015},
 title = {An Analysis of Heuristics in Othello},
 institution = {{University of Washington}}
}


@misc{vandenHerik.2002,
 author = {Herik, H.Jaap and Uiterwijk, Jos W.H.M. and Rijswijck, Jack},
 year = {2002},
 title = {Games solved: Now and in the future}
 }


@book{vanHerik.2008,
 year = {2008},
 title = {Computers and games: 6th international conference, CG 2008, Beijing, China, September 29-October 1, 2008 ; proceedings},
 price = {ca. EUR 68.43 (freier Pr.), ca. sfr 106.50 (freier Pr.)},
 address = {Berlin and Heidelberg},
 volume = {5131},
 publisher = {Springer},
 series = {Lecture Notes in Computer Science},
 editor = {den {van Herik}, Jaap and den {van Herik}, H. Jaap and Xu, Xinhe and Ma, Zongmin and Winands, Mark H. M.},
}


@misc{Winands.2008,
 author = {Winands, Mark H. M. and Bj{\"o}rnsson, Yngvi and Saito, Jahn-Takeshi},
 title = {Monte-Carlo Tree Search Solver},
 year = {2008},
}


@misc{Winands.2010,
 author = {Winands, Mark H. M. and Bjornsson, Yngvi and Saito, Jahn-Takeshi},
 year = {2010},
 title = {Monte Carlo Tree Search in Lines of Action},
 file = {Winands, Bjornsson et al. 2010 - Monte Carlo Tree Search:Attachments/Winands, Bjornsson et al. 2010 - Monte Carlo Tree Search.pdf:application/pdf}
}


